{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load USPS on Python 3.x\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = training_data\n",
    "X_cv, y_cv = validation_data\n",
    "X_test, y_test = test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(y):\n",
    "    y_OH = np.zeros((y.shape[0],10))\n",
    "    for i in range(y.shape[0]):\n",
    "        y_OH[i,y[i]]=1\n",
    "    return y_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_sanity_check(y,y_OH):\n",
    "    for i in range(y.shape[0]):\n",
    "        if np.argmax(y_OH[i])!=y[i]:\n",
    "            print('sanity check failed at index: '+str(i))\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_term(X):\n",
    "    return(np.concatenate((np.ones((X.shape[0],1)),X),axis=1)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_OH = getOneHot(y_train)\n",
    "y_cv_OH    = getOneHot(y_cv)\n",
    "y_test_OH  = getOneHot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_sanity_check(y_train,y_train_OH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_sanity_check(y_cv,y_cv_OH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_sanity_check(y_test,y_test_OH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_USPS = np.array(USPSMat)\n",
    "y_USPS = np.array(USPSTar)\n",
    "\n",
    "y_USPS_OH = getOneHot(y_USPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "NUM_HIDDEN_NEURONS_LAYER_1 = 800\n",
    "NUM_HIDDEN_NEURONS_LAYER_2 = 800\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "graph = tf.Graph()    # Instantiate a graph instance.\n",
    "with graph.as_default():  # Makes graph as the default graph.\n",
    "    '''\n",
    "    we initialize input and output tensors as placeholders. A placeholder initialization just reserves\n",
    "    the memory space as per the data type and shape of data, whose actual values will be provided later via feed dictionary.\n",
    "    This enables us to supply different data sets or data batches for training or testing at runtime.\n",
    "    '''\n",
    "    inputTensor  = tf.placeholder(tf.float32, [None, (28*28)])\n",
    "    outputTensor = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    ''' We use the tf.variable to create variable tensors(tensors whose values can be changed) to store weights \n",
    "    because the value of weights need to be updated by the optimizer during training'''\n",
    "    \n",
    "    hidden_layer1_weights = tf.Variable(tf.random_normal([(28*28),NUM_HIDDEN_NEURONS_LAYER_1],stddev=0.01))\n",
    "    hidden_layer1_biases = tf.Variable(tf.zeros(NUM_HIDDEN_NEURONS_LAYER_1))\n",
    "    \n",
    "    hidden_layer2_weights = tf.Variable(tf.random_normal([NUM_HIDDEN_NEURONS_LAYER_1,NUM_HIDDEN_NEURONS_LAYER_2],stddev=0.01))\n",
    "    hidden_layer2_biases = tf.Variable(tf.zeros(NUM_HIDDEN_NEURONS_LAYER_2))\n",
    "    \n",
    "    output_layer_weights = tf.Variable(tf.random_normal([NUM_HIDDEN_NEURONS_LAYER_2,10],stddev=0.01))\n",
    "    output_layer_biases = tf.Variable(tf.zeros(10))\n",
    "     \n",
    "    ''' We use the relu as activation functin for each layer. For any input value x,relu produces a value max(0,x).\n",
    "        Each hidden layer node computes the RELU of matrix multiplication of weights and the corresponding input from the previous layers, \n",
    "        as we build a fully connected neural network. The values thus produced act as inputs for the next layer.\n",
    "    '''\n",
    "    hidden_layer1_values = tf.nn.relu(tf.matmul(inputTensor,hidden_layer1_weights)+hidden_layer1_biases)\n",
    "    hidden_layer2_values = tf.nn.relu(tf.matmul(hidden_layer1_values,hidden_layer2_weights)+hidden_layer2_biases)\n",
    "    logits = tf.matmul(hidden_layer2_values,output_layer_weights)+output_layer_biases\n",
    "    # Logits are the values that are produced by the output nodes. \n",
    "    # After applying softmax function on them, they produce probabilities which determine the class to which that input data belongs. The selected class is the one with highest probability  \n",
    "    # The loss function determines the error between the predicted value and the actual value, using the cross entropy function in this case.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=outputTensor))\n",
    "    # In order to train the model, we run the optimizer(gradient descent in this case with a learning rate of 0.5).\n",
    "    # By training, we mean that the optimizer updates the values of weights and biases for every layer till the most optimal values are reached. \n",
    "    training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    # this function chooses the class by choosing the index of the class for which the softmax yields the highest probability.\n",
    "    prediction = tf.argmax(tf.nn.softmax(logits),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4177896cb64504b2f44c3059a66499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_OF_EPOCHS = 3000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "training_accuracy = []\n",
    "loss_list = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in tqdm_notebook(range(NUM_OF_EPOCHS)):\n",
    "        # Start batch training\n",
    "        offset = (step * BATCH_SIZE) % (y_train_OH.shape[0] - BATCH_SIZE)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = X_train[offset:(offset + BATCH_SIZE), :]\n",
    "        batch_labels = y_train_OH[offset:(offset + BATCH_SIZE), :]\n",
    "        sess.run(training, feed_dict={inputTensor: batch_data, \n",
    "                                          outputTensor: batch_labels})\n",
    "        \n",
    "\n",
    "    # Training accuracy for a dropout value in range 0 to 1\n",
    "    training_accuracy.append(np.mean(np.argmax(y_train_OH, axis=1) ==\n",
    "                                 sess.run(prediction, feed_dict={inputTensor: X_train,\n",
    "                                                                 outputTensor: y_train_OH})))\n",
    "    loss_list.append(sess.run(loss, feed_dict={inputTensor: X_train,\n",
    "                                                                 outputTensor: y_train_OH}))   \n",
    "    #Testing\n",
    "    predicted_mnist_test = sess.run(prediction, feed_dict={inputTensor: X_test})\n",
    "    predicted_usps = sess.run(prediction, feed_dict={inputTensor: X_USPS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99384]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy.\n",
    "np.mean(predicted_mnist_test==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.018650109]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5077753887694385"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted_usps==y_USPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
